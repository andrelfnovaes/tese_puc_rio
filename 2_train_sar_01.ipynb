{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility: set seeds for...\n",
    "seed_number = 0\n",
    "np.random.seed(seed_number) # numpy\n",
    "random.seed(seed_number) # Python's built-in random number generator\n",
    "tf.random.set_seed(seed_number) #tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_split = [\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "data_name = 'm6' # 'm6' or 'djia'\n",
    "\n",
    "# Data preprocessing\n",
    "index_col = 0\n",
    "shrink = 0.1\n",
    "feature_range = (0, 1)\n",
    "train_test_split = 0.8\n",
    "\n",
    "# Maximal window size\n",
    "max_window = 5\n",
    "\n",
    "# Load data\n",
    "data_file = 'data_original_'+str(data_name)+'.csv'\n",
    "data_original = pd.read_csv(data_file, index_col=index_col)\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments\n",
    "\n",
    "# Windows\n",
    "look_back_new = list(range(1, max_window+1))\n",
    "look_front_new = look_back_new\n",
    "\n",
    "look_back_orig = [int(x*2) for x in look_back_new]\n",
    "look_front_orig = [0] * len(look_back_orig)\n",
    "\n",
    "back_list = list(itertools.chain(*zip(look_back_new, look_back_orig)))\n",
    "front_list = list(itertools.chain(*zip(look_front_new, look_front_orig)))\n",
    "\n",
    "# Initialize data structures\n",
    "results = pd.DataFrame(columns=['ticker', 'look_back', 'look_front', 'trainScore'])\n",
    "\n",
    "j = 0\n",
    "# Iterate over tickers_split\n",
    "for tickers in tickers_split:\n",
    "\n",
    "    # Iterate over tickers\n",
    "    for ticker in tickers:\n",
    "\n",
    "        # Iterate over back and front windows\n",
    "        for look_back, look_front in zip(back_list, front_list):\n",
    "\n",
    "            # Shrink data\n",
    "            nrows = int(len(data_original) * shrink)\n",
    "            data = data_original.iloc[:nrows][ticker]\n",
    "\n",
    "            # Preprocess data\n",
    "            scaler = MinMaxScaler(feature_range=feature_range)\n",
    "            data = scaler.fit_transform(data.values.reshape(-1, 1))\n",
    "\n",
    "            # Split data into train and test sets\n",
    "            train_size = int(len(data) * train_test_split)\n",
    "            test_size = len(data) - train_size\n",
    "            train, test = data[0:train_size,:], data[train_size:len(data),:]\n",
    "\n",
    "            # Create training data\n",
    "            X_train, Y_train = [], []\n",
    "            for i in range(look_back, len(train) - look_front):\n",
    "                a = train[i - look_back:i, 0]\n",
    "                b = train[i:i + look_front, 0]\n",
    "                X_train.append(np.concatenate((a, b)))\n",
    "                Y_train.append(train[i, 0])\n",
    "            trainX_orig, trainY = np.array(X_train), np.array(Y_train)\n",
    "\n",
    "            # Reshape data input to be [samples, time steps, features]\n",
    "            trainX = np.reshape(trainX_orig, (trainX_orig.shape[0], 1, trainX_orig.shape[1]))\n",
    "\n",
    "            # Build model\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(lstm_units, input_shape=(trainX.shape[0], look_back+look_front)))\n",
    "            model.add(Dense(1))\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "            # Train model\n",
    "            model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "            # Make predictions\n",
    "            trainPredict = model.predict(trainX)\n",
    "\n",
    "            # Invert predictions\n",
    "            trainPredict = scaler.inverse_transform(trainPredict)\n",
    "            trainY = scaler.inverse_transform([trainY])\n",
    "\n",
    "            # Calculate root mean squared error\n",
    "            trainScore = np.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "            print('Train Score: %.2f RMSE' % (trainScore))\n",
    "\n",
    "            # Store the detailed results for the current value\n",
    "            results.loc[len(results)] = [ticker, look_back, look_front, trainScore]\n",
    "\n",
    "            # Save model\n",
    "            j = j + 1\n",
    "            model.save_weights(f'models/m_{j}_{ticker}_{look_back}_{look_front}_.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the DataFrame to get every other row starting from the first row\n",
    "results_btf = results.iloc[::2].reset_index(drop=True)\n",
    "\n",
    "# Slice the DataFrame to get every other row starting from the second row\n",
    "results_normal = results.iloc[1::2].reset_index(drop=True)\n",
    "\n",
    "# Create a new DataFrame with the given expressions\n",
    "df_diff = pd.DataFrame({\n",
    "    'trainScore_diff': np.round(((results_btf['trainScore']/results_normal['trainScore'] - 1)*100), 4),\n",
    "})\n",
    "\n",
    "def color_cells(val):\n",
    "    color = 'red' if val < 0 else 'green'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "# Apply the color_cells function to df_diff\n",
    "df_diff_styled = df_diff.style.applymap(color_cells)\n",
    "print('Value of the Future (VoF), the percentual difference from original windowing to BtF on RMSE:\\n')\n",
    "print('(RED values = BTF beats original windowing strategy)')\n",
    "display(df_diff_styled)\n",
    "\n",
    "print(f'\\nNumber of negative elements: {(df_diff <= 0).sum().sum()} over {df_diff.shape[0]*df_diff.shape[1]} elements ({round(((df_diff <= 0).sum().sum() / (df_diff.shape[0]*df_diff.shape[1])) * 100, 2)} %)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of repeated elemens\n",
    "\n",
    "counts = df_diff.stack().value_counts()\n",
    "df_counts = counts.to_frame(name='counts')\n",
    "display(df_counts[df_counts['counts'] > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributions and Hypothesis Testing\n",
    "\n",
    "pvalue = 0.05\n",
    "\n",
    "for to_plot in ['trainScore']:\n",
    "\n",
    "    # Extract the 'trainScore' columns\n",
    "    score_btf = results_btf[to_plot]\n",
    "    score_normal = results_normal[to_plot]\n",
    "\n",
    "    # Calculate the means and medians\n",
    "    mean_btf = score_btf.mean()\n",
    "    mean_normal = score_normal.mean()\n",
    "    median_btf = score_btf.median()\n",
    "    median_normal = score_normal.median()\n",
    "\n",
    "    # Create a figure\n",
    "    plt.figure()\n",
    "\n",
    "    label_BTF = 'BTF'\n",
    "    label_Normal = 'Classical Windowing'\n",
    "\n",
    "    # Plot histograms\n",
    "    plt.hist(score_btf, bins=30, color='blue', alpha=0.5, label=label_BTF)\n",
    "    plt.hist(score_normal, bins=30, color='green', alpha=0.5, label=label_Normal)\n",
    "\n",
    "    # Plot means\n",
    "    plt.axvline(mean_btf, color='darkblue', linestyle='dashed', linewidth=2, label='Mean of '+label_BTF)\n",
    "    plt.axvline(mean_normal, color='darkgreen', linestyle='dashed', linewidth=2, label='Mean of '+label_Normal)\n",
    "\n",
    "    # Plot medians\n",
    "    plt.axvline(median_btf, color='darkblue', linestyle='dotted', linewidth=2, label='Median of '+label_BTF)\n",
    "    plt.axvline(median_normal, color='darkgreen', linestyle='dotted', linewidth=2, label='Median of '+label_Normal)\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title(f'Distributions of {to_plot} Results')\n",
    "    plt.xlabel('trainScore')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "\n",
    "    # Initialize a list to store the results\n",
    "    results = []\n",
    "\n",
    "    # Perform the Mann-Whitney U Test\n",
    "    u_statistic, p_value = stats.mannwhitneyu(score_btf, score_normal)\n",
    "    results.append(['Mann-Whitney U Test', u_statistic, p_value, 'Reject H0' if p_value < pvalue else 'NOT Reject H0'])\n",
    "\n",
    "    # Perform the Kruskal-Wallis H Test\n",
    "    h_statistic, p_value = stats.kruskal(score_btf, score_normal)\n",
    "    results.append(['Kruskal-Wallis H Test', h_statistic, p_value, 'Reject H0' if p_value < pvalue else 'NOT Reject H0'])\n",
    "\n",
    "    # Perform the Wilcoxon Signed-Rank Test\n",
    "    # Note: This test requires the two samples to be paired and of the same size\n",
    "    if len(score_btf) == len(score_normal):\n",
    "        w_statistic, p_value = stats.wilcoxon(score_btf, score_normal)\n",
    "        results.append(['Wilcoxon Signed-Rank Test', w_statistic, p_value, 'Reject H0' if p_value < pvalue else 'NOT Reject H0'])\n",
    "    else:\n",
    "        results.append(['Wilcoxon Signed-Rank Test', None, None, 'Cannot perform test as the samples are not of the same size.'])\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    df_results = pd.DataFrame(results, columns=['Test', 'Statistic', 'P Value', 'Decision'])\n",
    "\n",
    "    # Display the DataFrame\n",
    "    display(df_results)\n",
    "\n",
    "    print('# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holistic-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
